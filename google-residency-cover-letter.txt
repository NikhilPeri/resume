# Question 1

My primary personal research interest is ML algorithms for video. Firstly I think the volume of video data will force us to come up with more efficient algorithms that can benefit the field as a whole.  But more generally I just find this data interesting since I think video is the closest digital representation of our reality and I see it as the intersection of all the problems in ML (spatial representations, sequence modeling, causality etc.)


# Question 2

Practically speaking with arxiv, meetup.com, and GCP I don’t think information or compute resources are any benefits I expect to get from this program.

In reality time is my limiting resource.  I work a full time job as a software engineer in data, but everything I have done up to this point has been using my spare time after work.  I have no shortage of interest and motivation but working 40 hours, commuting 10 hours and sleeping occasionally prevents me from committing fully to this work.  I think this program will allow me to spend more time working and thinking about on these types of problems I don’t get to touch in my work week.


# Question 3

The comma.ai speed challenge is to predict the speed of a vehicle given raw video data.  The problem was made more difficult since only 17 minutes of training data from a single drive video.  I would characterize this training dataset as small and non-diverse which is not a good combination.  I had to solve the problem of creating a reliable cross validation mechanism, and a model that is not prone to over fitting.

The naïve solution to cross validation is to randomly sample some percentage of frames to create an unseen validation set.  However, in the case of video adjacent frames share a high degree of similarity, as a result the validation set will not be an accurate measure of the models generalization ability.  First formalized this problem as measuring the “inter frame” distance between train and validation sets, which computes the distance between a validation frame and its nearest training frame.  I then experimented with different splitting strategies with the goal of maximizing inter-frame distance while not shifting the distribution of speed labels.  Ultimately, I found a simple “block based” approach to preform best.  This method takes the sequence of frames and breaks it into smaller blocks of fixed length, then each block is assigned to either the train or validation set, we then sample frames from each block into its respective set.  With this method I was able to improve interframe distance from 3 to 25 when compared to random splitting, while not changing the distribution of labels.

After developing a solution for reliable cross validation I tried to replicate the results of the Stanford Team http://cs229.stanford.edu/proj2017/final-reports/5244226.pdf and found their model was just overfitting and their claimed 91% accuracy was a result of random splitting of train and test.  Their solution was a VGG-19 model trained on adjacent frames from time t and t+1, however when looking at the saliency map for this model it was clear the model was memorizing landmarks of the short 17 min drive instead of generalizing.  Instead I decided to train a similar model on a semantic segmentation map and the dense optical flow.  First, I trained a unet on the Berkley Deep Drive Dataset which contains 7000 diverse road images with pixel level labels of 40 classes. However this number of labels was not necessary for our problem, instead I relabeled the data to into mobile (ex. cars, people) and fixed (ex. trees, bridges) then trained a binary unet. I also calculated the dense optical flow between the previous and current frame using Farneback's algorithm. Now instead of feeding rich colour images which contain many features prone to overfitting, my input was composed of three channels.  Two channels of dense optical flow which represents the motion of a pixel and a single channel representing whether the pixel is a part of a fixed or mobile object as classified from my unet.  I then replaced the VGG model with a simpler 5 layer CNN.  The combined model simplification and feature engineering allowed me to achieve a mean squared error of 6.05 on a truly unseen local validation set.

This is a brief explanation but a picture is worth a 256 dimensional embedding so if you would like to see my data visualizations check out the README.md https://github.com/NikhilPeri/speedchallenge
